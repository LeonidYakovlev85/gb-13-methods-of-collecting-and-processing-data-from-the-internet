# gb-13-methods-of-collecting-and-processing-data-from-the-internet
Домашние задания к курсу "Методы сбора и обработки данных из сети Интернет"
___

## Урок 1. Основы клиент-серверного взаимодействия. Парсинг API

1. Посмотреть документацию к API GitHub, разобраться как вывести список репозиториев для конкретного пользователя, сохранить JSON-вывод в файле *.json.

2. Изучить список открытых API. Найти среди них любое, требующее авторизацию (любого типа). Выполнить запросы к нему, пройдя авторизацию. Ответ сервера записать в файл.  
*В целях соблюдения конфиденциальности я удалил свой токен, но результаты запроса записаны в соответствующий файл*
___

## Урок 2. Парсинг HTML. Beautiful Soup

1. Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы) с сайтов Superjob и HH. Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:
    * наименование вакансии;
    * предлагаемую зарплату (отдельно минимальную и максимальную);
    * ссылку на саму вакансию;
    * сайт, откуда собрана вакансия.

 По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение). Структура должна быть одинаковая для вакансий с обоих сайтов. Общий результат можно вывести с помощью dataFrame через pandas.

 Можно выполнить по желанию один любой вариант или оба при желании и возможности.

### Пояснение к решению:

1. Количество страниц, в соответствии с рекомендацией преподавателя, задаётся через аргумент и равно пяти.
2. Для вывода данных о предполагаемой зарплате условия указаны не в полном объёме. В случае, если не указана "зарплатная вилка" и ни один из "порогов" ("от" или "до"), то значение зарплаты принимается и как минимальное, и как максимальное сразу.
3. К качестве условий выбран город Санкт-Петербург и профессия аналитик Big Data.
___

## Урок 3. Системы управления базами данных MongoDB и SQLite в Python

1. Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и реализовать функцию, записывающую собранные вакансии в созданную БД.
2. Написать функцию, которая производит поиск и выводит на экран вакансии с заработной платой больше введённой суммы.
3. Написать функцию, которая будет добавлять в вашу базу данных только новые вакансии с сайта.

### Пояснение к решению:
1. В решении использованы наработки предыдущего урока — получение вакансий с сайтов Superjob и HH.
    * Для обработки элементов каждого сайта создана отдельная функция.
    * Исправлена ошибка при обработке вакансий сайта HH, имеющих "зарплатную вилку" (был использован некорректный символ дефиса).
    * Исправлен механизм получения ссылки на вакансию для сайта HH.
    * Добавлен сбор информации об id вакансии.
    * Добавлен индикатор даты и времени внесения записи о вакансии.
2. В задании к предыдущему уроку речь шла о минимальном и максимальном размерах зарплаты, поэтому в функции, которая производит поиск и выводит на экран вакансии с заработной платой больше введённой суммы, сравнение по умолчанию будет производиться с минимальным размером зарплаты, в случае его отсутствия — с максимальным; вакансии с неуказанным размером зарплаты в сравнении не участвуют.
3. Не конкретизировано, что считать "новыми вакансиями" — вакансии, отсутствующие в базе, или вакансии, размещённые позднее определённого срока. Принимаю в качестве критерия "новизны" отсутствие вакансии в базе.
4. Так как id вакансии на обоих сайтах имеет одинаковую длину и теоретически они могут совпасть, то в качестве id БД MongoDB будет использоваться склейка из краткого имени сайта и id вакансии.
5. Не конкретизировано, какой из порогов зарплаты должен быть больше введённой суммы. Принимаю, что должен быть больше хотя бы один из порогов.
6. Вывод зарплаты производится не на основании условия "больше", а на основании "больше или равно".
___

## Урок 4. Парсинг HTML. XPath

1. Написать приложение, которое собирает основные новости с сайтов
    * https://news.mail.ru,
    * https://lenta.ru,
    * https://yandex.ru/news.
    
    Для парсинга использовать XPath. 

    Структура данных должна содержать:
    * название источника;
    * наименование новости;
    * ссылку на новость;
    * дата публикации.


2. Сложить собранные данные в БД

### Пояснение к решению:

1. Под основными новостями буду понимать новости главной страницы. Т. е. пагинация отсутствует.
2. В качестве названия источника принимаю полное доменное имя.
3. В случае news.mail.ru критерием новостной записи является наличие восьмизначного id. Записи из разделов «Справки» или «Карточки», имеющие трёхзначный id, новостями не являются и в сбор не включаются.
4. В случае lenta.ru критерием новостной записи является то, что она содержит url, начинающийся с «/news/», например: «news/2021/08/11/more_than_friends/. Записи с другими элементами («photo», «extlink», «brief», «articles», etc.) в сбор не включаются.
5. Важно отметить, что yandex.ru сам не формирует новости, а агрегирует их с других сайтов, поэтому итоговым источником будет строка типа «yandex.ru / «Имя Первоисточника»», а итоговой ссылкой на новость должна являться ссылка на первоисточник.
6. Получить ссылки на первоисточник для всех новостных записей yandex.ru не представляется возможным (в рамках данного проекта) в связи с блокировкой из-за превышения числа обращений.
___

## Урок 5. Scrapy

1. Доработать паука в имеющемся проекте, чтобы он складывал все записи в БД (любую) и формировал item по структуре:
    * наименование вакансии;
    * зарплата от;
    * зарплата до;
    * ссылка на саму вакансию;
    * сайт, откуда собрана вакансия.

2. Создать в имеющемся проекте второго паука по сбору вакансий с сайта superjob. Паук должен формировать item по аналогичной структуре и складывать данные в БД.
___

## Урок 6. Scrapy

(продолжение темы пятого урока)

3. Взять любую категорию товаров на сайте Леруа Мерлен. Собрать с использованием ItemLoader следующие данные:
    * название;
    * все фото;
    * параметры товара в объявлении.
    
4. С использованием output_processor и input_processor реализовать очистку и преобразование данных. Цены должны быть в виде числового значения.

5. *Написать универсальный обработчик параметров объявлений, который будет формировать данные вне зависимости от их типа и количества.

6. *Реализовать более удобную структуру для хранения скачиваемых фотографий.
___

## Урок 7. Selenium в Python

1. Написать программу, которая собирает входящие письма из своего или тестового почтового ящика, и сложить информацию о письмах в базу данных (от кого, дата отправки, тема письма, текст письма).

2. Написать программу, которая собирает «Хиты продаж» с сайтов техники М.видео, ОНЛАЙН ТРЕЙД и складывает данные в БД. Магазины можно выбрать свои. Главный критерий выбора: динамически загружаемые товары.

### Пояснение к решению:

1. Для решения первой задачи выбран ресурс mail.ru.

2. Для решения второй задачи выбран не онлайн магазин, а музыкальный ресурс genius.com. Принцип работы динамического окна налогичен сайтам, указанным в описании задачи. 

3. Между mail.ru и genius.com есть интересное различие в работе динамических окон: на genius.com информация после подгрузки сохраняется в DOM, а mail.ru держит в «памяти» только визуально отоброжаемые письма.
___
